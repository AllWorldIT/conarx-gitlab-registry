workflow:
  rules:
    # For merge requests, create a pipeline.
    - if: '$CI_MERGE_REQUEST_IID'
    # For the default branch, create a pipeline (this includes on schedules, pushes, merges, etc.).
    - if: '$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
    # For tags, create a pipeline.
    - if: '$CI_COMMIT_TAG'

default:
  image: golang:1.13-buster
  tags:
    - gitlab-org
  cache:
    key: ${CI_COMMIT_REF_SLUG}
    paths:
      - $GOPATH/pkg/mod/

variables:
  BUILDTAGS: "include_gcs include_oss"
  CGO_ENABLED: "1"

stages:
  - validate
  - test
  - integration

static-analysis:
  cache: {}
  image: registry.gitlab.com/gitlab-org/gitlab-build-images:golangci-lint-alpine
  stage: validate
  needs: []
  script:
    # Use default .golangci.yml file from the image if one is not present in the project root.
    - '[ -e .golangci.yml ] || cp /golangci/.golangci.yml .'
    # Write the code coverage report to gl-code-quality-report.json
    # and print linting issues to stdout in the format: path/to/file:line description
    - golangci-lint run --out-format code-climate | tee gl-code-quality-report.json | jq -r '.[] | "\(.location.path):\(.location.lines.begin) \(.description)"'
  artifacts:
    reports:
      codequality: gl-code-quality-report.json
    paths:
      - gl-code-quality-report.json
  allow_failure: true

bindata:
  cache: {}
  stage: validate
  needs: []
  before_script:
    - go get -u github.com/jteeuwen/go-bindata/...
  script: make bindata-check

verify:
  stage: test
  needs: ["bindata"]
  script:
    - go build -i .
    - make build
    - make binaries
    - make coverage

.storage-driver-test: &storage-driver-test
  stage: integration
  needs: ["bindata"]
  script: go test -v github.com/docker/distribution/registry/storage/driver/$CI_JOB_NAME -args -check.v -test.short

filesystem:
  <<: *storage-driver-test

inmemory:
  <<: *storage-driver-test

swift:
  <<: *storage-driver-test

s3-aws:
  <<: *storage-driver-test
  variables:
    AWS_ACCESS_KEY: "AKIAIOSFODNN7EXAMPLE"
    AWS_SECRET_KEY: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    MINIO_ACCESS_KEY: $AWS_ACCESS_KEY
    MINIO_SECRET_KEY: $AWS_SECRET_KEY
    REGION_ENDPOINT: "http://minio:9000"
    AWS_REGION: "us-east-2"
    S3_BUCKET: "test-bucket"
    S3_ENCRYPT: "false"
  services:
    - name: minio/minio:latest
      alias: "minio"
      command: ["server", "/data"]
  before_script:
    # Download the minio client
    - wget --no-verbose https://dl.min.io/client/mc/release/linux-amd64/mc
    - chmod u+x ./mc
    # Configure the minio client to use the local minio service rather than play.minio.io
    - ./mc config host add s3v4 $REGION_ENDPOINT $AWS_ACCESS_KEY $AWS_SECRET_KEY --api S3v4
    - ./mc mb s3v4/$S3_BUCKET

api:
  stage: integration
  needs: ["bindata"]
  script: go test -v -tags=integration github.com/docker/distribution/registry/handlers

.database: &database
  stage: integration
  needs: ["bindata"]
  variables:
    POSTGRES_PASSWORD: "secret"
    POSTGRES_DB: "registry_test"

    REGISTRY_DATABASE_ENABLED: "true"
    REGISTRY_DATABASE_HOST: "db"
    REGISTRY_DATABASE_PORT: "5432"
    REGISTRY_DATABASE_USER: "postgres"
    REGISTRY_DATABASE_PASSWORD: "secret"
    REGISTRY_DATABASE_DBNAME: "registry_test"
    REGISTRY_DATABASE_SSLMODE: "disable"
  services:
    - name: postgres:11-alpine
      alias: "db"
  script: go test -v -tags=integration $PACKAGE

database:datastore:
  <<: *database
  before_script:
    - export PACKAGE=github.com/docker/distribution/registry/datastore

database:api:
  <<: *database
  before_script:
    - export PACKAGE=github.com/docker/distribution/registry/handlers
