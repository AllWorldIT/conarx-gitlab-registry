# Container Registry CI tool

The CI Tool is used by the Container Registry team to assess the severity of CI flakes and identify trends in test failures.

It leverages JUnit data generated by our CI jobs and processes it via the GitLab API before storing it in a managed PostgreSQL database on AWS.
Data querying is conducted through Grafana.

The tool composes of:

- A Python script that runs periodically as a CI job
- A managed PostgreSQL database on AWS. Periodic data maintenance is done using the pg_cron and pg_partman extensions
- A Grafana dashboard which queries the database above and presents data in both graphical and tabular format

## Database

### Managed AWS instance

The processed CI flakes data is stored in a managed Postgres database in AWS.

A brief evaluation of GCE managed database was done, but it turned out that AWS offers more options for customizing the database like for example database port, storage size/IOPS, tuning database configuration and more.
THe database itself is running in [GitLab cloud sanbox](https://gitlabsandbox.cloud/) named `eng-dev-sandbox-prozlach`.

Unfortunately, we need to keep it exposed to the internet as GitLab runners can use any IP address assigned to GCE.
Automating a block-list would be too complex, so the decision was to use a non-standard port and user in order to mitigate bots scanners.

The credentials are stored in 1Password, look for entries starting with `Container Registry CI Tool database`.

The database has following configuration:

- engine version 16
- Instance class db.m5.large - 2 vCPU, 8GiB RAM
- no multi-AZ, no replication
- storage 40 GiB, general purpose ssd (gp3), 3000 IOPS
- backups kept for 7 days
- custom parameters:
  - `max_connections` set to 100
  - `max_parallel_workers_per_gather` set to 4
  - `pg_cron` extension added to `shared_preload_libraries`
  - `work_mem` set to 524288

### SQL code

We are using [Go-migrate](https://github.com/golang-migrate/migrate) tool to do database migrations.
Due to its limitations - lack of option to change connected database during migration, the migrations are split into two:

- `migrations_00-postgres` - migrations that must be done in the `postresql` database
- `migrations_01-ci_analitics` - migrations that must be done in the `ci_analitics` database

The procedure to re-create database code is as follows:

```shell
cd script/ci_tool
export PG_CONNECT_STRING="postgres://<psql_user>:<psql_pass>@<psql_host>:<psql_port>/postgres?sslmode=require"
/usr/bin/migrate -source file://db/migrations_00-postgres  -database "$PG_CONNECT_STRING" up
export PG_CONNECT_STRING="postgres://<psql_user>:<psql_pass>@<psql_host>:<psql_port>/ci_analitics?sslmode=require"
/usr/bin/migrate -source file://db/migrations_01-ci_analitics  -database "$PG_CONNECT_STRING" up
```

The setting up of the passwords is intentionally kept out of migrations code and needs to be done manually in order to keep secrets secret:

```sql
alter user ci_tool_writer with password '<psql_pass>';
alter user ci_tool_reader with password '<psql_pass>';
```

Please refer to the migration files to learn more about the schema.

At the time of writing this document, the database was less than 1GiB big.

### Automation

In order to keep the database compact and tidy, the database uses the pg_cron extension for periodic maintenance tasks.
Additionally, the database uses pg_partman extensions in order to split the data into partitions to make periodic removal of data faster and less disruptive.

The data is kept for 9 weeks, after that the partitions are dropped.
We also do a weekly cleanup of related tables in order to remove stale entries like e.g. test names.
